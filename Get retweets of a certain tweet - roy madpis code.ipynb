{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121a207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f637d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:04:49.924456Z",
     "start_time": "2022-03-22T14:04:45.090474Z"
    }
   },
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time, datetime\n",
    "import tweeterid\n",
    "\n",
    "# Helper class to convert a DynamoDB item to JSON.\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Decimal):\n",
    "            if o % 1 > 0:\n",
    "                return float(o)\n",
    "            else:\n",
    "                return int(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "    \n",
    "# academic token - the token of IDC\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff1b9ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:24:25.933465Z",
     "start_time": "2022-03-22T15:24:25.912476Z"
    }
   },
   "outputs": [],
   "source": [
    "### General function that are used in many functions we built:\n",
    "\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None, verbose = True):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    if verbose == True:\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def connect_to_endpoint_retweets(url, headers, params, next_token = None, verbose = True):\n",
    "    params['pagination_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    if verbose == True:\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a331ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:04:50.057938Z",
     "start_time": "2022-03-22T14:04:50.036384Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_url_retweet_by_tweet_id(tweet_id, max_results = 10):\n",
    "\n",
    "    #search_url = \"https://api.twitter.com/1.1/statuses/retweets/:id.json\" #get only the 100 last retweets of a given tweet id\n",
    "\n",
    "    search_url = search_url.replace(\":id\", tweet_id)\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'max_results': max_results,\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "########################################################### use:\n",
    "#search_url, query_params = create_url_tweet_id(search_url, tweet_id)\n",
    "#connect_to_endpoint(url, headers, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7aa76",
   "metadata": {},
   "source": [
    "### function get retweets of a certain tweet id\n",
    "The Following function enables getting all the retweets of a certain tweet\n",
    "\n",
    "arguments:\n",
    "\n",
    "+ **tweet_id** - The tweet id you wish to get its retweets\n",
    "+ **max_results** - The max amount of retweets you wish to get\n",
    "+ **next_token** - The token to look from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01cadae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:04:50.137412Z",
     "start_time": "2022-03-22T14:04:50.124420Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_url_retweets_by_tweet_id(tweet_id, max_results = 10, next_token = None):\n",
    "\n",
    "    #search_url = \"https://api.twitter.com/1.1/statuses/retweets/:id.json\" #get only the 100 last retweets of a given tweet id\n",
    "    search_url = \"https://api.twitter.com/2/tweets/:id/retweeted_by\"\n",
    "    search_url = search_url.replace(\":id\", tweet_id)\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {\n",
    "                    'expansions': 'pinned_tweet_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    #'place.fields': 'contained_within, country, country_code, full_name, geo, id, name, place_type',\n",
    "                    'max_results': max_results,\n",
    "                    'pagination_token': {next_token}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "########################################################### use:\n",
    "#search_url, query_params = create_url_tweet_id(search_url, tweet_id)\n",
    "#connect_to_endpoint(url, headers, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2185eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = create_headers(os.environ['TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d8637f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:04:59.857839Z",
     "start_time": "2022-03-22T14:04:59.167756Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n"
     ]
    }
   ],
   "source": [
    "headers = create_headers(os.environ['TOKEN'])\n",
    "search_url, query_params = create_url_retweets_by_tweet_id(tweet_id = \"1473447665757310980\") #\"1475817472414650369\"\n",
    "json_response = connect_to_endpoint(search_url, headers, query_params) #new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b05fb",
   "metadata": {},
   "source": [
    "\n",
    "+ We want to create **a dir for each tweet_id** (for each conversation id) this dir will contain the tree of that converation id\n",
    "\n",
    "Each such dir will contain the following:\n",
    "- excel file with all the retweets \n",
    "- json file with all the retweets\n",
    "- log file with retweet-tokens\n",
    "- log file with the number of retweets read\n",
    "\n",
    "#Function arguments explain:\n",
    "\n",
    "+ **dir_name** -The dir name to store for each tweet-id all its retweets\n",
    "+ **max_result** - check the maximum amount!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7e0da35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:50:49.602146Z",
     "start_time": "2022-03-22T14:50:49.596144Z"
    }
   },
   "outputs": [],
   "source": [
    "        #making dir to store, for each tweet-id all its retweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c0a1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:24:39.459930Z",
     "start_time": "2022-03-22T15:24:39.371982Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_retweets_of_tweet_id_SMALL(tweet_id=None,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_retweets = 10000000,\n",
    "                                   verbose = False, dir_tree_name = \"conversation_trees\"):\n",
    "\n",
    "    search_url = \"https://api.twitter.com/2/tweets/:id/retweeted_by\" #endpoint use to collect data from\n",
    "    search_url = search_url.replace(\":id\", tweet_id)\n",
    "\n",
    "    import os.path\n",
    "    #making a dir for the tree - this file will cintain a unique file for each conversation id\n",
    "    #dir_tree_name = \"conversation_trees\"\n",
    "    try:\n",
    "        os.mkdir(dir_tree_name)\n",
    "        print(\"creating tree directory\", dir_tree_name, \"to store all the trees\")\n",
    "    except:\n",
    "        print(\"The dir\", dir_tree_name ,\"already exist\")\n",
    "    \n",
    "    \n",
    "    #making dir (inside the tree dir) to store, for each tweet-id all its retweets\n",
    "    name_for_tweet_id = \"conv_tree_for_\" + str(tweet_id)\n",
    "    dir_name_for_tweet_id = os.path.join(dir_tree_name, name_for_tweet_id) \n",
    "    try:\n",
    "        os.mkdir(dir_name_for_tweet_id)\n",
    "        print(\"creating directory\", dir_name_for_tweet_id, \"to insert all the retweets of the given tweet-id\")\n",
    "    except:\n",
    "        print(\"The dir\", dir_name_for_tweet_id ,\"already exist\")\n",
    "\n",
    "    ##### the log dir\n",
    "    dir_log_name = os.path.join(dir_name_for_tweet_id, \"log_retweets_for_tweet_id_\" + tweet_id) \n",
    "    try:\n",
    "        os.mkdir(dir_log_name)\n",
    "        print(\"creating directory\", dir_log_name, \"to insert all the logs of the retweets for the tweet id - \", str(tweet_id))\n",
    "    except:\n",
    "        print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        \n",
    "    ########################\n",
    "#     path_for_log_dir_of_certain_user = os.path.join(dir_log_name, user_name)\n",
    "#     try:\n",
    "#         os.mkdir(path_for_log_dir_of_certain_user)\n",
    "#         print(\"creating directory\", path_for_log_dir_of_certain_user,\"in the dir\",dir_log_name, \"to insert all the logs of the key opinion leader\", user_name)\n",
    "#     except:\n",
    "#         print(\"The dir\", path_for_log_dir_of_certain_user ,\"already exist\")\n",
    "\n",
    "    path_for_dir_retriving_retweets_stream = os.path.join(dir_log_name, 'retriving_retweets_streem.txt')\n",
    "    with open(path_for_dir_retriving_retweets_stream, 'a') as f:\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "        current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "        f.write(current_time + '\\n\\n')\n",
    "\n",
    "    ########### If the token file exist already, then take the last token available, else start from token 1  ############ \n",
    "    tokens_location = os.path.join(dir_log_name, \"tokens.txt\") \n",
    "\n",
    "    if (evaluate_last_token == True and os.path.isfile(tokens_location) == True):\n",
    "        a_file = open(tokens_location, \"r\")\n",
    "        lines = a_file.readlines()\n",
    "        last_lines = lines[-2]\n",
    "        next_token = last_lines[0:-1]\n",
    "        a_file.close()    \n",
    "    else:\n",
    "        next_token = None\n",
    "\n",
    "    ################ Add a time stamp ########################################\n",
    "    with open(tokens_location, 'a') as f:\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "        current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "        f.write(current_time+ '\\n\\n')\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    continue_searching = True\n",
    "    json_response_list = []\n",
    "    next_tokens = []\n",
    "    num_of_returned_retweets = 0\n",
    "    counter_loops = 0\n",
    "\n",
    "    while continue_searching == True and num_of_returned_retweets < limit_amount_of_returned_retweets:\n",
    "        counter_loops +=1\n",
    "        if counter_loops > 1:\n",
    "            next_token = json_response[\"meta\"][\"next_token\"]\n",
    "            query_params[\"pagination_token\"] = next_token\n",
    "            print(\"token to insert:\",next_token)\n",
    "        #if the returned amount of retweets is getting close to the limit number, we need to alter the max_result,\n",
    "        #so we won't get retweets beyond what we asked\n",
    "        #if (limit_amount_of_returned_retweets - num_of_returned_retweets) < max_results:\n",
    "        max_results = min(limit_amount_of_returned_retweets - num_of_returned_retweets,max_results)\n",
    "        #else :\n",
    "        #    max_results = max_results\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "        query_params = {\n",
    "                        'expansions': 'pinned_tweet_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        #'place.fields': 'contained_within, country, country_code, full_name, geo, id, name, place_type',\n",
    "                        'max_results': max_results,\n",
    "                        'pagination_token': next_token}\n",
    "\n",
    "        \n",
    "        json_response = connect_to_endpoint_retweets(headers=headers,url = search_url, params= query_params, next_token = next_token)\n",
    "\n",
    "        json_response_list.append(json_response) #the first json_response itme\n",
    "        num_of_returned_retweets += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "        ##### making a dataframe out of the json response:\n",
    "        try:\n",
    "            a = pd.json_normalize(json_response[\"data\"])\n",
    "            a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "            list_of_cols_to_add = [\"id\",\"id_new\", \"verified\", \"username\", \"name\", \"created_at\", \"public_metrics.followers_count\",\n",
    "                                   \"public_metrics.following_count\", \"public_metrics.tweet_count\",\n",
    "                                   \"public_metrics.listed_count\", \"pinned_tweet_id\"]\n",
    "            list_cols_to_drop = [x for x in a.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "            ##droping labels we don't need\n",
    "            df_tweets_i = a.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "            for col in list_of_cols_to_add:\n",
    "                if col not in df_tweets_i.columns:\n",
    "                    df_tweets_i[col] = \"NA\"\n",
    "\n",
    "            #sort columns by alphabetic order\n",
    "            col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "            col_list_df_tweets_i.sort()\n",
    "            df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "\n",
    "            name = tweet_id + \".csv\"\n",
    "            path_for_table = os.path.join(dir_name_for_tweet_id, name)\n",
    "            if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                df_tweets_i.to_csv(path_for_table, index=True)\n",
    "            else:\n",
    "                df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "        except:\n",
    "            print(\"no data / include in the json\")\n",
    "\n",
    "        path_for_dir_all_json_responses = os.path.join(dir_log_name, 'all_json_responses.json')\n",
    "        with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "            json.dump(json_response_list, outfile)\n",
    "\n",
    "        with open(path_for_dir_retriving_retweets_stream, 'a') as f:\n",
    "            print_stat = str(counter_loops) + \" -> Got from twitter \" + str(json_response[\"meta\"][\"result_count\"]) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "            f.write(print_stat+'\\n')\n",
    "            print_total = \"Total amount of tweets: \" + str(num_of_returned_retweets)\n",
    "            f.write(print_total+ '\\n\\n')\n",
    "\n",
    "        if \"next_token\" in json_response[\"meta\"]:\n",
    "            if (verbose == True and counter_loops % 20 == 1):\n",
    "                print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "            elif verbose == False:\n",
    "                print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "            next_token = json_response[\"meta\"][\"next_token\"]\n",
    "            query_params[\"pagination_token\"] = next_token\n",
    "            next_tokens.append(next_token)\n",
    "            #ids_token_print = \"next token = \" + next_token + \"newest id: \" + json_response[\"meta\"][\"newest_id\"] + \" | oldest id: \" + json_response[\"meta\"][\"oldest_id\"]\n",
    "            ids_token_print = next_token\n",
    "            with open(tokens_location, 'a') as f:\n",
    "                f.write(ids_token_print + '\\n\\n')\n",
    "        else:\n",
    "            print(\"no more tweets from this user\")\n",
    "            continue_searching = False\n",
    "            print(\"Total amount of collected tweets = \", num_of_returned_retweets)\n",
    "\n",
    "        if num_of_returned_retweets >=limit_amount_of_returned_retweets:\n",
    "            print(\"oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\")\n",
    "            print(\"infact you got\", num_of_returned_retweets, \"returned tweets and limited the function to get\", limit_amount_of_returned_retweets, \"tweets\")\n",
    "\n",
    "    #In what case we suspect that there may be more tweets that we didn't get? -->\n",
    "    #When the number of tweets we asked to get is equal to the number of tweets we got back\n",
    "\n",
    "    ### save all thr json responses in json file:\n",
    "\n",
    "    return json_response_list, num_of_returned_retweets, next_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3899d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:24:47.743432Z",
     "start_time": "2022-03-22T15:24:41.641776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tree directory conversation_trees to store all the trees\n",
      "creating directory conversation_trees/conv_tree_for_1473447665757310980 to insert all the retweets of the given tweet-id\n",
      "creating directory conversation_trees/conv_tree_for_1473447665757310980/log_retweets_for_tweet_id_1473447665757310980 to insert all the logs of the retweets for the tweet id -  1473447665757310980\n",
      "Endpoint Response Code: 200\n",
      "1 Got from twitter 10 tweets, and there are more tweets of that user to get, I am bringing more tweets!\n",
      "\n",
      "token to insert: 7140dibdnow9c7btw480y1pt2iosnkae791k6wk5880fo\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1pi734vtidfash9us1zan2uq\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1p7blvfiydf3nk53rrwhzxh3\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1olzu82wrdwf75yg3jbshq2e\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1ob5wnlh7cssxex09jyj020i\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1o0gi3ew90whsxqp8hotzvhu\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480y1npmkj0piaqkznxebhouqvjq\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480xzldo3ha8zwtbrp3ljocoscvr\n",
      "Endpoint Response Code: 200\n",
      "token to insert: 7140dibdnow9c7btw480xzl2x7d5m3mly1i73ajfbp9gz\n",
      "Endpoint Response Code: 200\n",
      "oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\n",
      "infact you got 100 returned tweets and limited the function to get 100 tweets\n"
     ]
    }
   ],
   "source": [
    "tweet_id = \"1473447665757310980\"\n",
    "\n",
    "json_response_list, num_of_returned_retweets, next_tokens = return_retweets_of_tweet_id_SMALL(tweet_id=tweet_id,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_retweets = 100,\n",
    "                                   verbose = True, dir_tree_name = \"conversation_trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae49b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f8cef91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:14:40.183146Z",
     "start_time": "2022-03-22T15:14:40.096180Z"
    }
   },
   "outputs": [],
   "source": [
    "def __return_retweets_of_tweet_id_SMALL(self, tweet_id=None,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_retweets = 10000000,\n",
    "                                   verbose = False, dir_tree_name = \"conversation_trees\"):\n",
    "\n",
    "    search_url = \"https://api.twitter.com/2/tweets/:id/retweeted_by\" #endpoint use to collect data from\n",
    "    search_url = search_url.replace(\":id\", tweet_id)\n",
    "\n",
    "    import os.path\n",
    "    #making a dir for the tree - this file will cintain a unique file for each conversation id\n",
    "    #dir_tree_name = \"conversation_trees\"\n",
    "    try:\n",
    "        os.mkdir(dir_tree_name)\n",
    "        print(\"creating tree directory\", dir_tree_name, \"to store all the trees\")\n",
    "    except:\n",
    "        print(\"The dir\", dir_tree_name ,\"already exist\")\n",
    "    \n",
    "    \n",
    "    #making dir (inside the tree dir) to store, for each tweet-id all its retweets\n",
    "    name_for_tweet_id = \"conv_tree_for_\" + str(tweet_id)\n",
    "    dir_name_for_tweet_id = os.path.join(dir_tree_name, name_for_tweet_id) \n",
    "    try:\n",
    "        os.mkdir(dir_name_for_tweet_id)\n",
    "        print(\"creating directory\", dir_name_for_tweet_id, \"to insert all the retweets of the given tweet-id\")\n",
    "    except:\n",
    "        print(\"The dir\", dir_name_for_tweet_id ,\"already exist\")\n",
    "\n",
    "    ##### the log dir\n",
    "    dir_log_name = os.path.join(dir_name_for_tweet_id, \"log_retweets_for_tweet_id_\" + tweet_id) \n",
    "    try:\n",
    "        os.mkdir(dir_log_name)\n",
    "        print(\"creating directory\", dir_log_name, \"to insert all the logs of the retweets for the tweet id - \", str(tweet_id))\n",
    "    except:\n",
    "        print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        \n",
    "    ########################\n",
    "#     path_for_log_dir_of_certain_user = os.path.join(dir_log_name, user_name)\n",
    "#     try:\n",
    "#         os.mkdir(path_for_log_dir_of_certain_user)\n",
    "#         print(\"creating directory\", path_for_log_dir_of_certain_user,\"in the dir\",dir_log_name, \"to insert all the logs of the key opinion leader\", user_name)\n",
    "#     except:\n",
    "#         print(\"The dir\", path_for_log_dir_of_certain_user ,\"already exist\")\n",
    "\n",
    "    path_for_dir_retriving_retweets_stream = os.path.join(dir_log_name, 'retriving_retweets_streem.txt')\n",
    "    with open(path_for_dir_retriving_retweets_stream, 'a') as f:\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "        current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "        f.write(current_time + '\\n\\n')\n",
    "\n",
    "    ########### If the token file exist already, then take the last token available, else start from token 1  ############ \n",
    "    tokens_location = os.path.join(dir_log_name, \"tokens.txt\") \n",
    "\n",
    "    if (evaluate_last_token == True and os.path.isfile(tokens_location) == True):\n",
    "        a_file = open(tokens_location, \"r\")\n",
    "        lines = a_file.readlines()\n",
    "        last_lines = lines[-2]\n",
    "        next_token = last_lines[0:-1]\n",
    "        a_file.close()    \n",
    "    else:\n",
    "        next_token = None\n",
    "\n",
    "    ################ Add a time stamp ########################################\n",
    "    with open(tokens_location, 'a') as f:\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "        current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "        f.write(current_time+ '\\n\\n')\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    continue_searching = True\n",
    "    json_response_list = []\n",
    "    next_tokens = []\n",
    "    num_of_returned_retweets = 0\n",
    "    counter_loops = 0\n",
    "\n",
    "    while continue_searching == True and num_of_returned_retweets < limit_amount_of_returned_retweets:\n",
    "        counter_loops +=1\n",
    "        if counter_loops > 1:\n",
    "            next_token = json_response[\"meta\"][\"next_token\"]\n",
    "            query_params[\"pagination_token\"] = next_token\n",
    "            print(\"token to insert:\",next_token)\n",
    "        #if the returned amount of retweets is getting close to the limit number, we need to alter the max_result,\n",
    "        #so we won't get retweets beyond what we asked\n",
    "        if (limit_amount_of_returned_retweets - num_of_returned_retweets) < max_results:\n",
    "            max_results = limit_amount_of_returned_retweets - num_of_returned_retweets\n",
    "        else :\n",
    "            max_results = max_results\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "        query_params = {\n",
    "                        'expansions': 'pinned_tweet_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        #'place.fields': 'contained_within, country, country_code, full_name, geo, id, name, place_type',\n",
    "                        'max_results': max_results,\n",
    "                        'pagination_token': next_token}\n",
    "\n",
    "        \n",
    "        json_response = self.__connect_to_endpoint_retweets(url = search_url, params= query_params, next_token = next_token)\n",
    "\n",
    "        json_response_list.append(json_response) #the first json_response itme\n",
    "        num_of_returned_retweets += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "        ##### making a dataframe out of the json response:\n",
    "        try:\n",
    "            a = pd.json_normalize(json_response[\"data\"])\n",
    "            a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "            list_of_cols_to_add = [\"id\",\"id_new\", \"verified\", \"username\", \"name\", \"created_at\", \"public_metrics.followers_count\",\n",
    "                                   \"public_metrics.following_count\", \"public_metrics.tweet_count\",\n",
    "                                   \"public_metrics.listed_count\", \"pinned_tweet_id\"]\n",
    "            list_cols_to_drop = [x for x in a.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "            ##droping labels we don't need\n",
    "            df_tweets_i = a.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "            for col in list_of_cols_to_add:\n",
    "                if col not in df_tweets_i.columns:\n",
    "                    df_tweets_i[col] = \"NA\"\n",
    "\n",
    "            #sort columns by alphabetic order\n",
    "            col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "            col_list_df_tweets_i.sort()\n",
    "            df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "\n",
    "            name = tweet_id + \".csv\"\n",
    "            path_for_table = os.path.join(dir_name_for_tweet_id, name)\n",
    "            if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                df_tweets_i.to_csv(path_for_table, index=True)\n",
    "            else:\n",
    "                df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "        except:\n",
    "            print(\"no data / include in the json\")\n",
    "\n",
    "        with open(path_for_dir_retriving_retweets_stream, 'a') as f:\n",
    "            print_stat = str(counter_loops) + \" -> Got from twitter \" + str(json_response[\"meta\"][\"result_count\"]) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "            f.write(print_stat+'\\n')\n",
    "            print_total = \"Total amount of tweets: \" + str(num_of_returned_retweets)\n",
    "            f.write(print_total+ '\\n\\n')\n",
    "\n",
    "        if \"next_token\" in json_response[\"meta\"]:\n",
    "            if (verbose == True and counter_loops % 20 == 1):\n",
    "                print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "            elif verbose == False:\n",
    "                print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "            next_token = json_response[\"meta\"][\"next_token\"]\n",
    "            query_params[\"pagination_token\"] = next_token\n",
    "            next_tokens.append(next_token)\n",
    "            #ids_token_print = \"next token = \" + next_token + \"newest id: \" + json_response[\"meta\"][\"newest_id\"] + \" | oldest id: \" + json_response[\"meta\"][\"oldest_id\"]\n",
    "            ids_token_print = next_token\n",
    "            with open(tokens_location, 'a') as f:\n",
    "                f.write(ids_token_print + '\\n\\n')\n",
    "        else:\n",
    "            print(\"no more tweets from this user\")\n",
    "            continue_searching = False\n",
    "            print(\"Total amount of collected tweets = \", num_of_returned_retweets)\n",
    "\n",
    "        if num_of_returned_retweets >=limit_amount_of_returned_retweets:\n",
    "            print(\"oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\")\n",
    "            print(\"infact you got\", num_of_returned_retweets, \"returned tweets and limited the function to get\", limit_amount_of_returned_retweets, \"tweets\")\n",
    "\n",
    "    #In what case we suspect that there may be more tweets that we didn't get? -->\n",
    "    #When the number of tweets we asked to get is equal to the number of tweets we got back\n",
    "\n",
    "    ### save all thr json responses in json file:\n",
    "    path_for_dir_all_json_responses = os.path.join(dir_log_name, 'all_json_responses.json')\n",
    "    with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "        json.dump(json_response_list, outfile)\n",
    "    return json_response_list, num_of_returned_retweets, next_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5379dd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:21:55.320318Z",
     "start_time": "2022-03-22T15:21:55.312322Z"
    }
   },
   "outputs": [],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24783a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T15:21:33.306758Z",
     "start_time": "2022-03-22T15:21:33.297764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_params[\"pagination_token\"] = {next_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7de93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __return_tweets_of_key_opinion_leader(self, query=\"\", user_name=None,\n",
    "                                        start_time = \"2015-12-7T00:00:00Z\",\n",
    "                                        end_time = \"2021-12-26T00:00:00Z\",\n",
    "                                        max_results = 10, evaluate_last_token = False,\n",
    "                                        limit_amount_of_returned_tweets = 10000000,\n",
    "                                       verbose_10 = False, dir_name = \"key_opinion_leaders_tweets_tables_beta\"\n",
    "):\n",
    "  \n",
    "        search_url = \"https://api.twitter.com/2/tweets/search/all\" #endpoint use to collect data from\n",
    "\n",
    "        import os.path\n",
    "        try:\n",
    "            os.mkdir(dir_name)\n",
    "            print(\"creating directory\", dir_name, \"to insert all the tables of all the key opinion leaders\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_name ,\"already exist\")\n",
    "\n",
    "        if user_name is not None:\n",
    "            query = str(query) + \" from:\" + str(user_name)\n",
    "            try:\n",
    "                display_start_time = datetime.strptime(start_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "                display_end_time = datetime.strptime(end_time.split(\"T\")[0], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "            except:\n",
    "                display_start_time = start_time\n",
    "                display_end_time = end_time\n",
    "            print(\"Bringing all the tweets of the user:\", user_name, \"from:\", display_start_time, \"to\", display_end_time)\n",
    "            print()\n",
    "\n",
    "        ##### the log dir\n",
    "        import os.path\n",
    "        #dir_name = \"key_opinion_leaders_tweets_tables_beta\"\n",
    "        dir_name_beta = \"key_opinion_leaders_tweets_tables_beta\"\n",
    "        dir_log_name = os.path.join(dir_name_beta, \"log_key_opinion_leaders\") \n",
    "\n",
    "        # try:\n",
    "        #     os.mkdir(dir_name)\n",
    "        #     print(\"creating directory\", dir_name, \"to insert all the tables of all the key opinion leaders\")\n",
    "        # except:\n",
    "        #     print(\"The dir\", dir_name ,\"already exist\")\n",
    "        try:\n",
    "            os.mkdir(dir_log_name)\n",
    "            print(\"creating directory\", dir_log_name, \"to insert all the logs of the key opinion leaders\")\n",
    "        except:\n",
    "            print(\"The dir\", dir_log_name ,\"already exist\")\n",
    "        ########################\n",
    "        path_for_log_dir_of_certain_user = os.path.join(dir_log_name, user_name)\n",
    "        try:\n",
    "            os.mkdir(path_for_log_dir_of_certain_user)\n",
    "            print(\"creating directory\", path_for_log_dir_of_certain_user,\"in the dir\",dir_log_name, \"to insert all the logs of the key opinion leader\", user_name)\n",
    "        except:\n",
    "            print(\"The dir\", path_for_log_dir_of_certain_user ,\"already exist\")\n",
    "            \n",
    "        path_for_dir_retriving_tweets_streem = os.path.join(path_for_log_dir_of_certain_user, 'retriving_tweets_streem.txt')\n",
    "        with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "            current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "            f.write(current_time + '\\n\\n')\n",
    "            \n",
    "        ########### If the token file exist already, then take the last token available, else start from token 1  ############ \n",
    "        tokens_location = os.path.join(dir_log_name, user_name, \"tokens.txt\") \n",
    "\n",
    "        if (evaluate_last_token == True and os.path.isfile(tokens_location) == True):\n",
    "            a_file = open(tokens_location, \"r\")\n",
    "            lines = a_file.readlines()\n",
    "            last_lines = lines[-2]\n",
    "            next_token = last_lines[0:-1]\n",
    "            a_file.close()    \n",
    "        else:\n",
    "            next_token = None\n",
    "            \n",
    "        ################ Add a time stamp ########################################\n",
    "        path_for_dir_tokens = os.path.join(path_for_log_dir_of_certain_user, 'tokens.txt')\n",
    "        with open(path_for_dir_tokens, 'a') as f:\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S (Date: %d.%m.%y)\")\n",
    "            current_time = \"Current Time: \" + current_time + \"   *****************************************   \"\n",
    "            f.write(current_time+ '\\n\\n')\n",
    "            \n",
    "        ##########################################################################################\n",
    "\n",
    "        continue_searching = True\n",
    "        json_response_list = []\n",
    "        next_tokens = []\n",
    "        num_of_returned_tweets = 0\n",
    "        counter_loops = 0\n",
    "        \n",
    "        while continue_searching == True and num_of_returned_tweets < limit_amount_of_returned_tweets:\n",
    "            counter_loops +=1\n",
    "            if counter_loops > 1:\n",
    "                next_token = json_response[\"meta\"][\"next_token\"]\n",
    "                query_params[\"next_token\"] = next_token\n",
    "                print(\"token to insert:\",next_token)\n",
    "            #if the returned amount of tweets is getting close to the limit number, we need to alter the max_result,\n",
    "            #so we won't get tweets beyond what we asked\n",
    "            \n",
    "            if (limit_amount_of_returned_tweets - num_of_returned_tweets) < max_results:\n",
    "                max_results = limit_amount_of_returned_tweets - num_of_returned_tweets\n",
    "                if max_results < 10:\n",
    "                    max_results = 10\n",
    "            else :\n",
    "                max_results = max_results\n",
    "\n",
    "            #change params based on the endpoint you are using\n",
    "            query_params = {'query': query,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,entities.mentions.username,referenced_tweets.id',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {next_token}}\n",
    "            \n",
    "            json_response = self.__connect_to_endpoint(url = search_url,params= query_params, next_token = next_token)\n",
    "            \n",
    "            json_response_list.append(json_response) #the first json_response itme\n",
    "            num_of_returned_tweets += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "            ##### making a dataframe out of the json response:\n",
    "            try:\n",
    "                a = pd.json_normalize(json_response[\"data\"])\n",
    "                b = pd.json_normalize(json_response[\"includes\"], [\"users\"]).add_prefix(\"users.\")\n",
    "                \n",
    "                a.conversation_id = a.conversation_id.astype(\"string\")\n",
    "                a.id = a.id.astype(\"string\")\n",
    "                a[\"id_new\"] = \"id: \" + a[\"id\"].astype(\"string\")\n",
    "                a[\"conv_id_new\"] = \"conv_id: \" + a[\"conversation_id\"].astype(\"string\")\n",
    "            #c = pd.json_normalize(json_response[\"includes\"][\"places\"]).add_prefix(\"places.\")\n",
    "                df_tweets_i = pd.merge(a, b, left_on=\"author_id\", right_on=\"users.id\")\n",
    "                list_of_cols_to_add = ['author_id', 'conversation_id', \"conv_id_new\", \"id\", \"id_new\",\n",
    "                                    'created_at','entities.mentions',\n",
    "                            'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                        'public_metrics.reply_count', 'public_metrics.retweet_count','referenced_tweets', 'text',\n",
    "                            'users.created_at', 'users.description','users.id', 'users.name',\n",
    "                        'users.public_metrics.followers_count', 'users.public_metrics.following_count',\n",
    "                        'users.public_metrics.listed_count', 'users.public_metrics.tweet_count',\n",
    "                        'users.username', 'users.verified']\n",
    "\n",
    "                list_cols_to_drop = [x for x in df_tweets_i.columns if x not in list_of_cols_to_add]\n",
    "\n",
    "                ##droping labels we don't need\n",
    "                df_tweets_i = df_tweets_i.drop(labels=list_cols_to_drop, axis = 1, errors = \"ignore\")\n",
    "\n",
    "                for col in list_of_cols_to_add:\n",
    "                    if col not in df_tweets_i.columns:\n",
    "                        df_tweets_i[col] = \"NA\"\n",
    "\n",
    "                #sort columns by alphabetic order\n",
    "                col_list_df_tweets_i = df_tweets_i.columns.tolist()\n",
    "                col_list_df_tweets_i.sort()\n",
    "                df_tweets_i = df_tweets_i.reindex(columns=col_list_df_tweets_i)\n",
    "                \n",
    "                name = user_name + \".csv\"\n",
    "                path_for_table = os.path.join(dir_name_beta, name)\n",
    "                if os.path.isfile(path_for_table) == False: #if this is the first table of tweets\n",
    "                    df_tweets_i.to_csv(path_for_table, index=True)\n",
    "                else:\n",
    "                    df_tweets_i.to_csv(path_for_table, mode='a', index=True, header=False)\n",
    "            except:\n",
    "                print(\"no data / include in the json\")\n",
    "        \n",
    "            with open(path_for_dir_retriving_tweets_streem, 'a') as f:\n",
    "                print_stat = str(counter_loops) + \" -> Got from twitter \" + str(json_response[\"meta\"][\"result_count\"]) + \" tweets, and there are more tweets of that user to get, I am bringing more tweets!\"\n",
    "                f.write(print_stat+'\\n')\n",
    "                print_total = \"Total amount of tweets: \" + str(num_of_returned_tweets)\n",
    "                f.write(print_total+ '\\n\\n')\n",
    "\n",
    "            if \"next_token\" in json_response[\"meta\"]:\n",
    "                if (verbose_10 == True and counter_loops % 20 == 1):\n",
    "                    print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "                elif verbose_10 == False:\n",
    "                    print(counter_loops, \"Got from twitter\", json_response[\"meta\"][\"result_count\"], \"tweets, and there are more tweets of that user to get, I am bringing more tweets!\\n\")\n",
    "                next_token = json_response[\"meta\"][\"next_token\"]\n",
    "                query_params[\"next_token\"] = next_token\n",
    "                next_tokens.append(next_token)\n",
    "                #ids_token_print = \"next token = \" + next_token + \"newest id: \" + json_response[\"meta\"][\"newest_id\"] + \" | oldest id: \" + json_response[\"meta\"][\"oldest_id\"]\n",
    "                ids_token_print = next_token\n",
    "                with open(path_for_dir_tokens, 'a') as f:\n",
    "                    f.write(ids_token_print + '\\n\\n')\n",
    "            else:\n",
    "                print(\"no more tweets from this user\")\n",
    "                continue_searching = False\n",
    "                print(\"Total amount of collected tweets = \", num_of_returned_tweets)\n",
    "            \n",
    "            if num_of_returned_tweets >=limit_amount_of_returned_tweets:\n",
    "                print(\"oooops, There may be more tweets to return, but you asked to limit the amount of returned tweets\")\n",
    "                print(\"infact you got\", num_of_returned_tweets, \"returned tweets and limited the function to get\", limit_amount_of_returned_tweets, \"tweets\")\n",
    "        \n",
    "        #In what case we suspect that there may be more tweets that we didn't get? -->\n",
    "        #When the number of tweets we asked to get is equal to the number of tweets we got back\n",
    "        \n",
    "        ### save all thr json responses in json file:\n",
    "        path_for_dir_all_json_responses = os.path.join(path_for_log_dir_of_certain_user, 'all_json_responses.json')\n",
    "        with open(path_for_dir_all_json_responses, 'w') as outfile:\n",
    "            json.dump(json_response_list, outfile)\n",
    "        return json_response_list, num_of_returned_tweets, next_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ddede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_retweets_by_tweet_id_BIG(self, tweet_id=\"\", dir_name=\"retweets\", user_names =None, \\\n",
    "                                     max_results = 10,limit_amount_of_returned_retweets = 10000000, verbose = False):\n",
    "    \n",
    "\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_retweets_by_tweet_id_BIG(self, query=\"\", dir_name=\"tweets\", user_names =None, \\\n",
    "        start_time = \"2015-12-7T00:00:00Z\", end_time = \"2021-12-26T00:00:00Z\",\n",
    "        max_results = 10, evaluate_last_token = False, \\\n",
    "            limit_amount_of_returned_tweets = 10000000, verbose_10 = False):\n",
    "\n",
    "            if type(user_names) != list:\n",
    "                user_names = [user_names]\n",
    "            \n",
    "            #users_json_response_lists = []\n",
    "            names_evaluated = []\n",
    "            names_didnt_evaluated = []\n",
    "            next_tokens_users= [] #this will include a list where eachelement is a list containing all the tokens off the specific user\n",
    "            for name in user_names:\n",
    "                print(\"Bringing tweets of\", name)\n",
    "                query = \"\"\n",
    "                user_name = name\n",
    "                try:\n",
    "\n",
    "                    json_response_list, num_of_returned_tweets,next_tokens = self.__return_tweets_of_key_opinion_leader(query=query, user_name=user_name,\n",
    "                                                            start_time = start_time,\n",
    "                                                            end_time = end_time,\n",
    "                                                            max_results = max_results,\n",
    "                                                            limit_amount_of_returned_tweets = limit_amount_of_returned_tweets,\n",
    "                                                                                                                verbose_10 = True)\n",
    "                    print(num_of_returned_tweets)\n",
    "                    if num_of_returned_tweets > 0:\n",
    "                        names_evaluated.append(name)\n",
    "                        next_tokens_users.append(next_tokens)\n",
    "                \n",
    "\n",
    "                    else:\n",
    "                        names_didnt_evaluated.append(name)\n",
    "                        print(\"The user:\", name, \"had\", num_of_returned_tweets, \"tweets!!\")\n",
    "\n",
    "                    print(\"---------------------------------------------------------------\")\n",
    "                except:\n",
    "                    print(\"There was a problem with the key opinion leader:\", name)\n",
    "                    names_didnt_evaluated.append(name)\n",
    "                    print(\"*************************************************************************************\")\n",
    "\n",
    "    import time, datetime,json, numpy as np\n",
    "    from inputimeout import inputimeout, TimeoutOccurred\n",
    "\n",
    "\n",
    "    def create_url_tweet_ids(self, search_url, tweet_ids_list, verbose = False):\n",
    "    \n",
    "        #search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "        search_url = search_url.replace(\"X\", ','.join(tweet_ids_list))\n",
    "        if verbose: print(search_url)\n",
    "        #change params based on the endpoint you are using\n",
    "        query_params = {\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return (search_url, query_params)\n",
    "\n",
    "\n",
    "\n",
    "    #   sleep_time = 15*60 ## 15 minutes in sec\n",
    "    #   tweet_ids = all_harvard_data[\"ID\"].to_numpy().astype(str)\n",
    "\n",
    "    from os import path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f495a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:05:01.503784Z",
     "start_time": "2022-03-22T14:05:01.494788Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "includes\n",
      "errors\n",
      "meta\n"
     ]
    }
   ],
   "source": [
    "for i in json_response:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3810e365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T14:05:03.058027Z",
     "start_time": "2022-03-22T14:05:02.994065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1506200159604162562',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 1,\n",
       "   'following_count': 0,\n",
       "   'tweet_count': 242,\n",
       "   'listed_count': 0},\n",
       "  'username': 'JjusticeBieber',\n",
       "  'name': 'Justin Bieber',\n",
       "  'created_at': '2022-03-22T09:24:53.000Z',\n",
       "  'description': 'JUSTICE the new album out now'},\n",
       " {'pinned_tweet_id': '1405500123451301899',\n",
       "  'id': '945189278849445888',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 15,\n",
       "   'following_count': 142,\n",
       "   'tweet_count': 1615,\n",
       "   'listed_count': 0},\n",
       "  'username': 'iambellagella',\n",
       "  'name': 'BELLA BRUCE GELLA',\n",
       "  'created_at': '2017-12-25T07:07:39.000Z',\n",
       "  'description': '#annemarie @AnneMarie \\u200d#harrystyles @harry_styles \\u200diG:IAMBELLAGELLA'},\n",
       " {'id': '4828270962',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 57,\n",
       "   'following_count': 33,\n",
       "   'tweet_count': 773,\n",
       "   'listed_count': 0},\n",
       "  'username': 'GzlAranza',\n",
       "  'name': 'Aranza Paredes',\n",
       "  'created_at': '2016-01-29T02:14:23.000Z',\n",
       "  'description': ''},\n",
       " {'id': '1242209516361519105',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 6,\n",
       "   'following_count': 108,\n",
       "   'tweet_count': 138,\n",
       "   'listed_count': 0},\n",
       "  'username': 'nora2ray',\n",
       "  'name': '',\n",
       "  'created_at': '2020-03-23T22:00:18.000Z',\n",
       "  'description': ''},\n",
       " {'pinned_tweet_id': '1386643507889250304',\n",
       "  'id': '1224410682738860033',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 799,\n",
       "   'following_count': 357,\n",
       "   'tweet_count': 12563,\n",
       "   'listed_count': 0},\n",
       "  'username': 'Bellz_baby_',\n",
       "  'name': \"Nengi's comrade\",\n",
       "  'created_at': '2020-02-03T19:14:05.000Z',\n",
       "  'description': \"It's all love here \\n\\nAquarius| Ninja | 30BG | Everything nice and soft\"},\n",
       " {'id': '1970196229',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 473,\n",
       "   'following_count': 264,\n",
       "   'tweet_count': 17055,\n",
       "   'listed_count': 0},\n",
       "  'username': 'mirella_lima2',\n",
       "  'name': 'Mirella Fonseca',\n",
       "  'created_at': '2013-10-19T02:53:12.000Z',\n",
       "  'description': ''},\n",
       " {'id': '1301126836626358273',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 16,\n",
       "   'following_count': 221,\n",
       "   'tweet_count': 34,\n",
       "   'listed_count': 0},\n",
       "  'username': 'Sarah_neal01',\n",
       "  'name': 'SARAH NEAL',\n",
       "  'created_at': '2020-09-02T11:56:43.000Z',\n",
       "  'description': 'God  first, family love ones '},\n",
       " {'pinned_tweet_id': '1505928537320042502',\n",
       "  'id': '1505920368921366528',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 13,\n",
       "   'following_count': 19,\n",
       "   'tweet_count': 37,\n",
       "   'listed_count': 0},\n",
       "  'username': 'CCCreammy3',\n",
       "  'name': 'CCCreammy',\n",
       "  'created_at': '2022-03-21T14:53:01.000Z',\n",
       "  'description': 'Hi!'},\n",
       " {'id': '1276079520991281154',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 9,\n",
       "   'following_count': 75,\n",
       "   'tweet_count': 16,\n",
       "   'listed_count': 0},\n",
       "  'username': 'lonelysoulc',\n",
       "  'name': 'lonelysoul',\n",
       "  'created_at': '2020-06-25T09:07:41.000Z',\n",
       "  'description': ''},\n",
       " {'id': '1452341303400796160',\n",
       "  'verified': False,\n",
       "  'public_metrics': {'followers_count': 6,\n",
       "   'following_count': 6,\n",
       "   'tweet_count': 7,\n",
       "   'listed_count': 0},\n",
       "  'username': 'pikopapepopu',\n",
       "  'name': '',\n",
       "  'created_at': '2021-10-24T18:28:55.000Z',\n",
       "  'description': ''}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beddc069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:37:23.834860Z",
     "start_time": "2022-03-20T15:37:23.828862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: [{'value': '1496054215621369857', 'detail': 'Could not find tweet with pinned_tweet_id: [1496054215621369857].', 'title': 'Not Found Error', 'resource_type': 'tweet', 'parameter': 'pinned_tweet_id', 'resource_id': '1496054215621369857', 'type': 'https://api.twitter.com/2/problems/resource-not-found'}, {'value': '784938598319607808', 'detail': 'Could not find tweet with pinned_tweet_id: [784938598319607808].', 'title': 'Not Found Error', 'resource_type': 'tweet', 'parameter': 'pinned_tweet_id', 'resource_id': '784938598319607808', 'type': 'https://api.twitter.com/2/problems/resource-not-found'}, {'value': '1505012876813250562', 'detail': 'Could not find tweet with pinned_tweet_id: [1505012876813250562].', 'title': 'Not Found Error', 'resource_type': 'tweet', 'parameter': 'pinned_tweet_id', 'resource_id': '1505012876813250562', 'type': 'https://api.twitter.com/2/problems/resource-not-found'}]\n",
      "meta: {'result_count': 10, 'next_token': '7140dibdnow9c7btw480y1pi5jvhk12c0zerx4c5jlhyq'}\n"
     ]
    }
   ],
   "source": [
    "#print(\"data:\", json_response[\"data\"])\n",
    "#print(\"includes:\", json_response[\"includes\"])\n",
    "print(\"errors:\", json_response[\"errors\"])\n",
    "print(\"meta:\", json_response[\"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4904dd4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:38:26.413961Z",
     "start_time": "2022-03-20T15:38:26.406965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7140dibdnow9c7btw480y1pi5jvhk12c0zerx4c5jlhyq\n"
     ]
    }
   ],
   "source": [
    "next_token = json_response[\"meta\"][\"next_token\"] #here we extract the next token, we will use this token in a for loop\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a647c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9906df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fd1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f99552e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:38:37.069967Z",
     "start_time": "2022-03-20T15:38:36.560469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n"
     ]
    }
   ],
   "source": [
    "#### round 2:\n",
    "\n",
    "headers = create_headers(os.environ['TOKEN'])\n",
    "search_url, query_params = create_url_retweets_by_tweet_id(tweet_id = \"1473447665757310980\",\n",
    "                                                          max_results = 10, next_token = next_token) #\"1475817472414650369\"\n",
    "json_response = connect_to_endpoint(search_url, headers, query_params) #new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed85d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:39:03.726820Z",
     "start_time": "2022-03-20T15:39:03.719821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: {'result_count': 10, 'next_token': '7140dibdnow9c7btw480y1ownp3am92ahs30h4ihp7ozd', 'previous_token': '77qpymm88g5h9vqkluxceh7kd7q9mbibfzlfjka2vrsja'}\n"
     ]
    }
   ],
   "source": [
    "print(\"meta:\", json_response[\"meta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73b8ef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:38:38.068917Z",
     "start_time": "2022-03-20T15:38:38.061922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7140dibdnow9c7btw480y1ownp3am92ahs30h4ihp7ozd\n"
     ]
    }
   ],
   "source": [
    "next_token = json_response[\"meta\"][\"next_token\"]\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e5f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2376ae5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:27:36.407970Z",
     "start_time": "2022-03-20T15:27:36.399974Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [1046342533897490433]\n",
      "next_cursor: 0\n",
      "next_cursor_str: 0\n",
      "previous_cursor: 0\n",
      "previous_cursor_str: 0\n",
      "total_count: None\n"
     ]
    }
   ],
   "source": [
    "print(\"ids:\", json_response[\"ids\"])\n",
    "print(\"next_cursor:\", json_response[\"next_cursor\"])\n",
    "print(\"next_cursor_str:\", json_response[\"next_cursor_str\"])\n",
    "print(\"previous_cursor:\", json_response[\"previous_cursor\"])\n",
    "print(\"previous_cursor_str:\", json_response[\"previous_cursor_str\"])\n",
    "print(\"total_count:\", json_response[\"total_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "71c87fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:13:10.023941Z",
     "start_time": "2022-03-20T15:13:10.011941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_response[\"ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39953f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cf1457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:40:49.666524Z",
     "start_time": "2022-03-20T14:40:49.654529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.twitter.com/1.1/statuses/retweets/1475817472414650369.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_url = \"https://api.twitter.com/1.1/statuses/retweets/:id.json\"\n",
    "search_url = search_url.replace(\":id\", tweet_id)\n",
    "search_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237fc41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:40:58.543555Z",
     "start_time": "2022-03-20T14:40:58.508571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created_at': 'Sun Mar 20 14:37:54 +0000 2022',\n",
       "  'id': 1505554226566635524,\n",
       "  'id_str': '1505554226566635524',\n",
       "  'text': 'RT @roymadpis789: Trying Twitter for the first time! #fun #michael_kobaivanov',\n",
       "  'truncated': False,\n",
       "  'entities': {'hashtags': [{'text': 'fun', 'indices': [54, 58]},\n",
       "    {'text': 'michael_kobaivanov', 'indices': [59, 78]}],\n",
       "   'symbols': [],\n",
       "   'user_mentions': [{'screen_name': 'roymadpis789',\n",
       "     'name': 'Roy m',\n",
       "     'id': 1460609632813170693,\n",
       "     'id_str': '1460609632813170693',\n",
       "     'indices': [3, 16]}],\n",
       "   'urls': []},\n",
       "  'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       "  'in_reply_to_status_id': None,\n",
       "  'in_reply_to_status_id_str': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'in_reply_to_screen_name': None,\n",
       "  'user': {'id': 1460590577465147395,\n",
       "   'id_str': '1460590577465147395',\n",
       "   'name': 'Michael Kobaivanov',\n",
       "   'screen_name': 'MichaelKobaiva1',\n",
       "   'location': '',\n",
       "   'description': '',\n",
       "   'url': None,\n",
       "   'entities': {'description': {'urls': []}},\n",
       "   'protected': False,\n",
       "   'followers_count': 1,\n",
       "   'friends_count': 1,\n",
       "   'listed_count': 0,\n",
       "   'created_at': 'Tue Nov 16 12:48:51 +0000 2021',\n",
       "   'favourites_count': 1,\n",
       "   'utc_offset': None,\n",
       "   'time_zone': None,\n",
       "   'geo_enabled': True,\n",
       "   'verified': False,\n",
       "   'statuses_count': 13,\n",
       "   'lang': None,\n",
       "   'contributors_enabled': False,\n",
       "   'is_translator': False,\n",
       "   'is_translation_enabled': False,\n",
       "   'profile_background_color': 'F5F8FA',\n",
       "   'profile_background_image_url': None,\n",
       "   'profile_background_image_url_https': None,\n",
       "   'profile_background_tile': False,\n",
       "   'profile_image_url': 'http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png',\n",
       "   'profile_image_url_https': 'https://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png',\n",
       "   'profile_link_color': '1DA1F2',\n",
       "   'profile_sidebar_border_color': 'C0DEED',\n",
       "   'profile_sidebar_fill_color': 'DDEEF6',\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_use_background_image': True,\n",
       "   'has_extended_profile': True,\n",
       "   'default_profile': True,\n",
       "   'default_profile_image': True,\n",
       "   'following': None,\n",
       "   'follow_request_sent': None,\n",
       "   'notifications': None,\n",
       "   'translator_type': 'none',\n",
       "   'withheld_in_countries': []},\n",
       "  'geo': None,\n",
       "  'coordinates': None,\n",
       "  'place': None,\n",
       "  'contributors': None,\n",
       "  'retweeted_status': {'created_at': 'Tue Dec 28 13:14:39 +0000 2021',\n",
       "   'id': 1475817472414650369,\n",
       "   'id_str': '1475817472414650369',\n",
       "   'text': 'Trying Twitter for the first time! #fun #michael_kobaivanov',\n",
       "   'truncated': False,\n",
       "   'entities': {'hashtags': [{'text': 'fun', 'indices': [36, 40]},\n",
       "     {'text': 'michael_kobaivanov', 'indices': [41, 60]}],\n",
       "    'symbols': [],\n",
       "    'user_mentions': [],\n",
       "    'urls': []},\n",
       "   'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       "   'in_reply_to_status_id': None,\n",
       "   'in_reply_to_status_id_str': None,\n",
       "   'in_reply_to_user_id': None,\n",
       "   'in_reply_to_user_id_str': None,\n",
       "   'in_reply_to_screen_name': None,\n",
       "   'user': {'id': 1460609632813170693,\n",
       "    'id_str': '1460609632813170693',\n",
       "    'name': 'Roy m',\n",
       "    'screen_name': 'roymadpis789',\n",
       "    'location': '',\n",
       "    'description': '',\n",
       "    'url': None,\n",
       "    'entities': {'description': {'urls': []}},\n",
       "    'protected': False,\n",
       "    'followers_count': 2,\n",
       "    'friends_count': 7,\n",
       "    'listed_count': 0,\n",
       "    'created_at': 'Tue Nov 16 14:04:28 +0000 2021',\n",
       "    'favourites_count': 0,\n",
       "    'utc_offset': None,\n",
       "    'time_zone': None,\n",
       "    'geo_enabled': False,\n",
       "    'verified': False,\n",
       "    'statuses_count': 1,\n",
       "    'lang': None,\n",
       "    'contributors_enabled': False,\n",
       "    'is_translator': False,\n",
       "    'is_translation_enabled': False,\n",
       "    'profile_background_color': 'F5F8FA',\n",
       "    'profile_background_image_url': None,\n",
       "    'profile_background_image_url_https': None,\n",
       "    'profile_background_tile': False,\n",
       "    'profile_image_url': 'http://pbs.twimg.com/profile_images/1460609720243339274/MGPX5_xQ_normal.png',\n",
       "    'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1460609720243339274/MGPX5_xQ_normal.png',\n",
       "    'profile_link_color': '1DA1F2',\n",
       "    'profile_sidebar_border_color': 'C0DEED',\n",
       "    'profile_sidebar_fill_color': 'DDEEF6',\n",
       "    'profile_text_color': '333333',\n",
       "    'profile_use_background_image': True,\n",
       "    'has_extended_profile': True,\n",
       "    'default_profile': True,\n",
       "    'default_profile_image': False,\n",
       "    'following': None,\n",
       "    'follow_request_sent': None,\n",
       "    'notifications': None,\n",
       "    'translator_type': 'none',\n",
       "    'withheld_in_countries': []},\n",
       "   'geo': None,\n",
       "   'coordinates': None,\n",
       "   'place': None,\n",
       "   'contributors': None,\n",
       "   'is_quote_status': False,\n",
       "   'retweet_count': 1,\n",
       "   'favorite_count': 1,\n",
       "   'favorited': False,\n",
       "   'retweeted': False,\n",
       "   'lang': 'en'},\n",
       "  'is_quote_status': False,\n",
       "  'retweet_count': 1,\n",
       "  'favorite_count': 0,\n",
       "  'favorited': False,\n",
       "  'retweeted': False,\n",
       "  'lang': 'en'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "1475817472414650369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df39da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2a4170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:35:21.766842Z",
     "start_time": "2022-03-20T14:35:21.750848Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_url_tweet_id(search_url, tweet_id):\n",
    "    \n",
    "    #search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "    search_url = search_url.replace(\":id\", tweet_id)\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "########################################################### use:\n",
    "#search_url, query_params = create_url_tweet_id(search_url, tweet_id)\n",
    "#connect_to_endpoint(url, headers, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d349fa06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:35:26.039866Z",
     "start_time": "2022-03-20T14:35:25.465545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n"
     ]
    }
   ],
   "source": [
    "#search url for idtweet - App rate limit: 300 requests per 15-minute window shared among all users of your app\n",
    "#User rate limit (OAuth 1.0a): 900 requests per 15-minute window per each authenticated user\n",
    "headers = create_headers(os.environ['TOKEN'])\n",
    "search_url = \"https://api.twitter.com/2/tweets/:id\" #Change to the endpoint you want to collect data from\n",
    "tweet_id = \"1475817472414650369\" #\"1108157488581562373\"\n",
    "###-------------------------------------------------------------------------------------\n",
    "search_url, query_params = create_url_tweet_id(search_url, tweet_id)\n",
    "json_response = connect_to_endpoint(search_url, headers, query_params) #new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4c1660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:35:29.434099Z",
     "start_time": "2022-03-20T14:35:29.400114Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'lang': 'en',\n",
       "  'public_metrics': {'retweet_count': 1,\n",
       "   'reply_count': 1,\n",
       "   'like_count': 1,\n",
       "   'quote_count': 0},\n",
       "  'reply_settings': 'everyone',\n",
       "  'conversation_id': '1475817472414650369',\n",
       "  'created_at': '2021-12-28T13:14:39.000Z',\n",
       "  'author_id': '1460609632813170693',\n",
       "  'source': 'Twitter Web App',\n",
       "  'text': 'Trying Twitter for the first time! #fun #michael_kobaivanov',\n",
       "  'id': '1475817472414650369'},\n",
       " 'includes': {'users': [{'username': 'roymadpis789',\n",
       "    'id': '1460609632813170693',\n",
       "    'name': 'Roy m',\n",
       "    'public_metrics': {'followers_count': 2,\n",
       "     'following_count': 7,\n",
       "     'tweet_count': 1,\n",
       "     'listed_count': 0},\n",
       "    'created_at': '2021-11-16T14:04:28.000Z',\n",
       "    'verified': False,\n",
       "    'description': ''}]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a8498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7253ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0008d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a923eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847593c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
