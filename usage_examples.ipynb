{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060144454831689734\n",
      "@founder_startup\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install virtualenv\n",
    "#! py -3 -m venv brexit-env\n",
    "#! virtualenv brexit-env\n",
    "#! source brexit-env/bin/activate\n",
    "#! pip3 install -r requirements.txt\n",
    "\n",
    "import tweeterid\n",
    "\n",
    "print(tweeterid.handle_to_id('founder_startup'))\n",
    "print(tweeterid.id_to_handle('1060144454831689734'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_crawler import TwitterCrawler\n",
    "\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT'\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwitterCrawler.get_url_by_tweet_id(\"1466035318365294603\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = \"1470753200311517189\" #\"1108157488581562373\"\n",
    "my_twitter_crawler.create_url_tweet_id(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"fun\"\n",
    "start_time =\"2022-03-10T00:00:00.000Z\"\n",
    "end_time = \"2022-03-11T00:00:00.000Z\"\n",
    "max_results = 10 #minimum 10 and maximum 100 !\n",
    "\n",
    "my_twitter_crawler.search_recent_by_keyword(keyword, start_time,end_time, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing  search_recent_by_keyword\n",
    "\n",
    "\n",
    "all_key_opinion_leader_names = [\"DavidDavisMP\"]\n",
    "all_key_opinion_leader_names=[\"theresa_may\", \"PhilipHammondUK\", \"MartinSelmayr\", \"BorisJohnson\", \"David_Cameron\", \"BSTimBarrow\", \"AMCarwyn\", \"WalkerWorcester\"]\n",
    "\n",
    "#all_key_opinion_leader_names = [\"DominicCumins\", \"darrengrimes_\", \"PhilipHammondUK\",\"roymadpis789\"]\n",
    "#MakhoulRawaa\n",
    "start_time = \"2015-12-7T00:00:00Z\"\n",
    "end_time = \"2021-12-04T00:00:00Z\"\n",
    "max_results = 500\n",
    "limit_amount_of_returned_tweets = 1000000\n",
    "############################################################################################\n",
    "\n",
    "#### we don't want to get data on key opinion leaders that we already searched for their tweets!\n",
    "# thus we first need to see which of the users that are inserted in the list are **not** in the files we already have:\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#making the data and log dir\n",
    "dir_name = \"key_opinion_leaders_tweets_tables_beta\"\n",
    "#dir_log_name = \"log_key_opinion_leaders\"\n",
    "#path_for_log_dir = os.path.join(dir_name, dir_log_name)\n",
    "\n",
    "import os.path\n",
    "try:\n",
    "    os.mkdir(dir_name)\n",
    "    print(\"creating directory\", dir_name, \"to insert all the tables of all the key opinion leaders\")\n",
    "except:\n",
    "    print(\"The dir\", dir_name ,\"already exist\")\n",
    "\n",
    "    \n",
    "mypath = \"key_opinion_leaders_tweets_tables_beta\"\n",
    "users_we_have_data = [f.split(\".\")[0] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "key_opinion_leader_names = list(set(all_key_opinion_leader_names) - set(users_we_have_data))\n",
    "print(\"Users to search their tweets:\",key_opinion_leader_names)\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"Users we already have data on:\",list(set(all_key_opinion_leader_names).intersection(users_we_have_data)))\n",
    "\n",
    "\n",
    "json_responses_list, num_of_returned_twweets, next_token = \\\n",
    "my_twitter_crawler.return_tweets_of_key_opinion_leader(query=\"\", user_name=\"DavidDavisMP\",\n",
    "                                        start_time = \"2015-12-7T00:00:00Z\",\n",
    "                                        end_time = \"2021-12-26T00:00:00Z\",\n",
    "                                        max_results = 10, evaluate_last_token = False,\n",
    "                                        limit_amount_of_returned_tweets = 50,\n",
    "                                       verbose_10 = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "tokens_location = \"/Users/michaelkobaivanov/Library/Mobile Documents/com~apple~CloudDocs/year3/brexit-package/key_opinion_leaders_tweets_tables_beta/log_key_opinion_leaders/DavidDavisMP/tokens.txt\"\n",
    "\n",
    "\n",
    "a_file = open(tokens_location, \"r\")\n",
    "lines = a_file.readlines()\n",
    "last_lines = lines[-2]\n",
    "next_token = last_lines[0:-1]\n",
    "a_file.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_names=[\"theresa_may\", \"PhilipHammondUK\", \"MartinSelmayr\", \"BorisJohnson\", \"David_Cameron\", \"BSTimBarrow\", \"AMCarwyn\", \"WalkerWorcester\"]\n",
    "user_names=[\"MichelBarnier\"]\n",
    "\n",
    "my_twitter_crawler.return_tweets_of_key_opinion_leaders(query=\"\", user_names= user_names,\n",
    "                                        start_time = \"2015-01-01T00:00:00Z\",\n",
    "                                        end_time = \"2021-12-26T00:00:00Z\",\n",
    "                                        max_results = 500, evaluate_last_token = False,\n",
    "                                        limit_amount_of_returned_tweets = 1000000,\n",
    "                                       verbose_10 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_twitter_crawler.get_tweets_by_tweet_ids(tweet_ids= [\"682715594110689280\",\"682715872205651968\", \"682716086681382912\", \"682720118401581056\"],\n",
    "json_tweets_output_folder = \"json_tweets_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = [\"14734476657573\",\"1506035877751005193\"]\n",
    "\n",
    "my_twitter_crawler.return_retweets_by_tweet_ids(tweet_ids=tweet_id,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_retweets = 20,\n",
    "                                   verbose = True, dir_tree_name = \"conversation_trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_id = [\"14734476657573\",\"1506035877751005193\"]\n",
    "\n",
    "tweet_id = [\"14734476657573\", \"1506662230892314634\"]\n",
    "#tweet_id = [\"1506662230892314634\", \"1506035877751005193\"]\n",
    "my_twitter_crawler.return_quotes_by_tweet_ids(tweet_ids=tweet_id,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_quotes = 30,\n",
    "                                   verbose = True, dir_tree_name = \"conversation_trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_id = [\"14734476657573\",\"1506035877751005193\"]\n",
    "\n",
    "tweet_id = [\"14734476657573\", \"1506662230892314634\"]\n",
    "#tweet_id = [\"1506662230892314634\", \"1506035877751005193\"]\n",
    "my_twitter_crawler.return_likes_by_tweet_ids(tweet_ids=tweet_id,\n",
    "                                    max_results = 10, evaluate_last_token = False,\n",
    "                                    limit_amount_of_returned_likes = 30,\n",
    "                                   verbose = True, dir_tree_name = \"conversation_trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### comments\n",
    "from twitter_crawler import TwitterCrawler\n",
    "\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT'\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)\n",
    "\n",
    "\n",
    "conversation_ids = [\"1505554747335577605\",\"1506662230892314634\"]\n",
    "\n",
    "\n",
    "\n",
    "path_for_table_dict = my_twitter_crawler.return_comments_by_tweet_ids(conversation_ids = conversation_ids, query = \"\",\n",
    "                              start_time = \"2015-12-7T00:00:00Z\",\n",
    "                              end_time = \"today\",\n",
    "                              max_results = 10, evaluate_last_token = False,\n",
    "                              limit_amount_of_returned_comments = 1000,\n",
    "                              verbose = True, dir_tree_name = \"conversation_trees\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function filter tweets\n",
    "\n",
    "We read numerous amount of tweets, comments, retweets and quotes from twitter. \n",
    "Now we wish to create a function that, given a dataframe with  tweets / comments / quotes it will enable scoring them.\n",
    "The scoring will be based on the following parameters:\n",
    "\n",
    "1. The author - if he is one of the KOPs - K points\n",
    "2. The time - if it is in the day of key event - K points\n",
    "3. The text - if it contains key words - K points\n",
    "\n",
    "### Notes:\n",
    "+ Note that you can't mix tables with tweets / comments / quotes - this is because they have different amount of columns. You need to process the tweets, comments and quotes table separately\n",
    "\n",
    "+ **The key-events table and the KOP table - MUST BE OF XLSX TYPE!!**\n",
    "\n",
    "\n",
    "### The Filter function gets as input:\n",
    "\n",
    "+ 1. **dir_with_all_tweets** - The location / name of dir (if the dir is in the same location of the jupyter file) of all the tweets table that we want to read into the function (this is important as all the csv tweet tables that will be located in this location will be read into the function).\n",
    "\n",
    "+ 2. **score_for_KOP** = By default = 5. This argument controls the **weight** given to tweet with author id of **Key Opinion Leader**\n",
    "\n",
    "+ 3. **score_for_key_event** = By default = 5. This argument controls the **weight** given to tweet that was published in **key event day**.\n",
    "\n",
    "+ 4. **score_for_key_words** = By default = 5. This argument controls the **weight** given to **each** **key-word** that is located in the tweet.\n",
    "\n",
    "+ 5. **key_words** = By default = [\"brexit\", \"eu\", \"deal\"]. This argument controls the **Key words** to search in each tweet that are related to the brexit. Tweets with high amount of those words are assumed to be important.\n",
    "\n",
    "+ 6. **threshold_score** - By default = 10. This is a very important argument. It controls the minimum score that a tweet must get in order to be returned in the filtered tweets table. Any tweet with score under this value won't be included in the filtered table\n",
    "\n",
    "+ 7. **KOP_excel_name** - The name of the excel file (**xlsx format**) that contains the Key Opinion Leaders\n",
    "\n",
    "+ 8. **key_events_excel_name** - The name of the excel file (**xlsx format**) that contains the key events data\n",
    "\n",
    "+ 9. **stop_words_to_add** = A list with all the additional words you wish the function to handle as stop words (those words will be removed from each list of tokens for each tweet).\n",
    "\n",
    "+ 10. **stop_words_file_name** = The name of the text file that contains all the stopwords. Note that if it doesn't exist, the function will go to \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\" - to find the stopwords\n",
    "\n",
    "+ 11. **verbose** = by default = True. If True than the function will print its progress (suggested!)\n",
    "\n",
    "### The function returns:\n",
    "\n",
    "+ **tweets_table_filtered** - The filtered tweets table is a table containing all the tweets that their score is higher than the `threshold_score`\n",
    "\n",
    "+ **tweets_table** - The preprocessed tweets table with all the tweets - This table is identical to the `tweets_table_filtered` except that it also included tweets with score under the `threshold_score`\n",
    "\n",
    "+ **csv_files_evaluated** = a list with all the **csv** files that the function read and add to the tweets table (the files are read from the location you provided in the argument: `dir_with_all_tweets`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_crawler import TwitterCrawler\n",
    "\n",
    "my_token = 'AAAAAAAAAAAAAAAAAAAAADRFOwEAAAAAaTp%2Bdd1OhobYMYb5ExPXm7IL6RA%3DDHknH402gOXoegUGxNtpC6giIjdackRLtdRx6tjrnnLeFN1ntT'\n",
    "my_twitter_crawler = TwitterCrawler(bearer_token= my_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('.') ]\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs for the filter function\n",
    "dir_with_all_tweets = os.path.join(\"key_opinion_leaders_tweets_tables_beta\")\n",
    "\n",
    "score_for_KOP = 5\n",
    "score_for_key_event = 5\n",
    "score_for_key_words = 5\n",
    "\n",
    "key_words = [\"brexit\", \"eu\", \"deal\", \"economy\"]\n",
    "threshold_score = 10\n",
    "\n",
    "KOP_excel_name = \"KOP brexit.xlsx\"\n",
    "key_events_excel_name = \"Brexit_key_events.xlsx\"\n",
    "stop_words_to_add = [\"https\"]\n",
    "#########################################################\n",
    "tweets_table_filtered, tweets_table, csv_files_evaluated = my_twitter_crawler.filter_tweets_Brexit(\n",
    "    dir_with_all_tweets = dir_with_all_tweets,\n",
    "    score_for_KOP = score_for_KOP,\n",
    "    score_for_key_event = score_for_key_event,\n",
    "    score_for_key_words = score_for_key_words,\n",
    "    key_words = key_words,\n",
    "    threshold_score = threshold_score,\n",
    "    KOP_excel_name = KOP_excel_name,\n",
    "    key_events_excel_name = key_events_excel_name,\n",
    "    stop_words_to_add = stop_words_to_add,\n",
    "    stop_words_file_name = \"stopwords.txt\",\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweets_table_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_table_filtered.to_excel(\"tweets_table_filtered.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which files are processed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"csv files processed:\\n\")\n",
    "for index, file in enumerate(csv_files_evaluated):\n",
    "    print(index+1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See all the texts of the filtered tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tweets_table_filtered.text:\n",
    "    print (i)\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d3c51dcc737e3b755df5c6775e1f16aeea3921fc151dde7ea513fa4bec4ea0c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('brexit-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
